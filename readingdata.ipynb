{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Elapsed time: 0.24802207946777344 seconds\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>SEQN</th>\n","      <th>H1CD0006</th>\n","      <th>H1CD0012</th>\n","      <th>H1CD0013</th>\n","      <th>H1CD0014</th>\n","      <th>H1CD0015</th>\n","      <th>H1CD0016</th>\n","      <th>H1CD0017</th>\n","      <th>H1CD0018</th>\n","      <th>H1CD0019</th>\n","      <th>...</th>\n","      <th>H1CD0213</th>\n","      <th>H1CD0214</th>\n","      <th>H1CD0215</th>\n","      <th>H1CD0216</th>\n","      <th>H1CD0217</th>\n","      <th>H1CD0218</th>\n","      <th>H1CD0219</th>\n","      <th>H1CD0220</th>\n","      <th>H1CD0221</th>\n","      <th>H1CD0222</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>30001</td>\n","      <td>18009</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>30002</td>\n","      <td>18009</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>30003</td>\n","      <td>18425</td>\n","      <td>4</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>30004</td>\n","      <td>18425</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>30005</td>\n","      <td>25224</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>6667</th>\n","      <td>36668</td>\n","      <td>18278</td>\n","      <td>4</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>6668</th>\n","      <td>36669</td>\n","      <td>20270</td>\n","      <td>4</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>6669</th>\n","      <td>36670</td>\n","      <td>20544</td>\n","      <td>4</td>\n","      <td>5</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>6670</th>\n","      <td>36671</td>\n","      <td>23995</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>6671</th>\n","      <td>36672</td>\n","      <td>18278</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>6672 rows Ã— 191 columns</p>\n","</div>"],"text/plain":["       SEQN  H1CD0006  H1CD0012  H1CD0013  H1CD0014  H1CD0015  H1CD0016  \\\n","0     30001     18009         5         5         5         5         1   \n","1     30002     18009         5         5         5         5         2   \n","2     30003     18425         4         5         5         4         1   \n","3     30004     18425         5         5         5         5         2   \n","4     30005     25224         2         5         4         4         2   \n","...     ...       ...       ...       ...       ...       ...       ...   \n","6667  36668     18278         4         5         5         5         2   \n","6668  36669     20270         4         5         5         1         1   \n","6669  36670     20544         4         5         4         4         1   \n","6670  36671     23995         5         5         2         5         2   \n","6671  36672     18278         5         5         5         4         1   \n","\n","      H1CD0017  H1CD0018  H1CD0019  ...  H1CD0213  H1CD0214  H1CD0215  \\\n","0            5         3         2  ...         0         0         0   \n","1            5         3         2  ...         0         0         0   \n","2            5         3         2  ...         0         0         0   \n","3            5         3         2  ...         0         0         0   \n","4            5         3         2  ...         0         0         0   \n","...        ...       ...       ...  ...       ...       ...       ...   \n","6667         5         3         2  ...         0         0         0   \n","6668         5         3         2  ...         4         4         4   \n","6669         5         3         2  ...         0         0         0   \n","6670         5         3         2  ...         0         0         0   \n","6671         5         3         2  ...         0         0         0   \n","\n","      H1CD0216  H1CD0217  H1CD0218  H1CD0219  H1CD0220  H1CD0221  H1CD0222  \n","0            0         0         0         0         0         0       NaN  \n","1            0         0         0         0         0         0       NaN  \n","2            0         0         0         0         0         0       NaN  \n","3            0         0         0         0         0         0       NaN  \n","4            0         0         0         0         0         0       NaN  \n","...        ...       ...       ...       ...       ...       ...       ...  \n","6667         0         0         0         0         0         0       NaN  \n","6668         2         2         2         2         2         2       NaN  \n","6669         0         0         0         0         0         0       NaN  \n","6670         0         0         0         0         0         0       NaN  \n","6671         0         0         0         0         0         0       NaN  \n","\n","[6672 rows x 191 columns]"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import time\n","import numpy as np\n","start_time = time.time()\n","\n","df = pd.read_csv('readdata.csv', sep=',', low_memory=False)\n","\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","print(f\"Elapsed time: {elapsed_time} seconds\")\n","df"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["    SEQN  H1CD0006  H1CD0012  H1CD0013  H1CD0014  H1CD0015  H1CD0016  \\\n","0  30001     18009         5         5         5         5         1   \n","1  30002     18009         5         5         5         5         2   \n","2  30003     18425         4         5         5         4         1   \n","3  30004     18425         5         5         5         5         2   \n","4  30005     25224         2         5         4         4         2   \n","\n","   H1CD0017  H1CD0018  H1CD0019  ...  H1CD0213  H1CD0214  H1CD0215  H1CD0216  \\\n","0         5         3         2  ...         0         0         0         0   \n","1         5         3         2  ...         0         0         0         0   \n","2         5         3         2  ...         0         0         0         0   \n","3         5         3         2  ...         0         0         0         0   \n","4         5         3         2  ...         0         0         0         0   \n","\n","   H1CD0217  H1CD0218  H1CD0219  H1CD0220  H1CD0221  H1CD0222  \n","0         0         0         0         0         0       NaN  \n","1         0         0         0         0         0       NaN  \n","2         0         0         0         0         0       NaN  \n","3         0         0         0         0         0       NaN  \n","4         0         0         0         0         0       NaN  \n","\n","[5 rows x 191 columns]\n","Index(['H1CD0222'], dtype='object')\n"]}],"source":["nan_rows = df[df.isnull().any(axis=1)]\n","#print(nan_rows.head())\n","\n","nan_cols_labels = df.columns[df.isnull().any()]\n","#print(nan_cols_labels)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Time of Examination(01-21)           14.0\n","Average Systolic Blood Pressure     125.0\n","Average Diastolic Blood Pressure     78.0\n","EKG-READING-AVERAGE RATE             75.0\n","EKG READING-AVERAGE PR INTERVAL      16.0\n","EKG READING-AVERAGE INTERVAL          7.0\n","S-SAVE DEPTH IN V1 (SV1)            100.0\n","R-SAVE DEPTH IN V5 (SVS)            130.0\n","dtype: float64\n"]}],"source":["\n","# create a dictionary to map old column names to new column names\n","column_mapping = {\n","    'SEQN': 'Sequence Number',\n","    'H1CD0006': 'Final Estimation Weight',\n","    'H1CD0012': 'Headaches',\n","    'H1CD0013': 'Nose Bleeds',\n","    'H1CD0014': 'Tinnitus',\n","    'H1CD0015': 'Dizziness',\n","    'H1CD0016': 'Fainting',\n","    'H1CD0017': 'Stroke',\n","    'H1CD0018': 'Stroke Review Summary',\n","    'H1CD0019': 'Paralyzed',\n","    'H1CD0020': 'Sore Throats',\n","    'H1CD0021': 'Shortness of Breath When Climbing Stairs',\n","    'H1CD0022': 'Shortness of Breath When Doing Physi ...',\n","    'H1CD0023': 'Shortness of Breath When Idle',\n","    'H1CD0024': 'Shortness of Breath When Excited or  ...',\n","    'H1CD0025': 'Wake Up at Night Because of Shortnes ...',\n","    'H1CD0026': 'Chest Pain-In Past Few Years',\n","    'H1CD0027': 'Chest Pain-Where Does It Bother You?',\n","    'H1CD0028': 'Chest Pain-Does It Move Around?',\n","    'H1CD0029': 'Chest Pain-Duration?',\n","    'H1CD0030': 'Chest Pain-When Does It Usually Come?(exercise)',\n","    'H1CD0031': 'Chest Pain-When Does It Usually Come?(mood)',\n","    'H1CD0032': 'Chest Pain-Take Medication for It?',\n","    'H1CD0033': 'Heart Pain-In Part Few Year?',\n","    'H1CD0034': 'Heart Pain-Where Does It Bother You?',\n","    'H1CD0035': 'Heart Pain-Does It Move Around:',\n","    'H1CD0036': 'Heart Pain-Duration?',\n","    'H1CD0037': 'Heart Pain-When Does It Usually Come?(exercise)',\n","    'H1CD0038': 'Heart Pain-When Does It Usually Come?(mood)',\n","    'H1CD0039': 'Hear Pain-Late Medication for It?',\n","    'H1CD0040': 'Heart Beat-Acting Funny?',\n","    'H1CD0041': 'Heart Beat-Beating Hard?',\n","    'H1CD0042': 'Swollen Ankles at Bedtime?',\n","    'H1CD0043': 'Leg Cramps?',\n","    'H1CD0044': 'Varicose Veins?',\n","    'H1CD0045': 'Rheumatic Fever Diagnosis?',\n","    'H1CD0046': 'Chorea or St. Vitus Dance Diagnosis?',\n","    'H1CD0047': 'Hardening of the Arteries Diagnosis?',\n","    'H1CD0048': 'High Blood Pressure-Think You Have It?',\n","    'H1CD0049': 'High Blood Pressure- How Long Ago Di ...',\n","    'H1CD0050': 'High Blood Pressure-Had It Within Pa ...',\n","    'H1CD0051': 'High Blood Pressure-Take Medication?',\n","    'H1CD0052': 'Heart Trouble-Think You Have It?',\n","    'H1CD0053': 'Heart Trouble-How Long Did It Start?',\n","    'H1CD0054': 'Heart Trouble-Had It Within Past 12  ...',\n","    'H1CD0055': 'Heart Trouble-Take Medication?',\n","    'H1CD0056': 'Time of Examination(01-21)',\n","    'H1CD0058': 'Average Systolic Blood Pressure',\n","    'H1CD0061': 'Average Diastolic Blood Pressure',\n","    'H1CD0064': 'Average Systolic Blood Pressure Recode',\n","    'H1CD0066': 'Average Diastolic Blood Pressure Recode',\n","    'H1CD0068': 'When Venipuncture Was Done in Relati ...',\n","    'H1CD0069': 'Ocular Fundi-Right Eye',\n","    'H1CD0070': 'Ocular Fundi-Left Eye',\n","    'H1CD0071': 'Ocular Fundi Condition-Right Eye',\n","    'H1CD0072': 'Ocular Fundi Condition-Left Eye',\n","    'H1CD0073': 'Venous Engorgement-Right Eye',\n","    'H1CD0074': 'Venous Engorgement-Left Eye',\n","    'H1CD0075': 'Disc. Abnormality-Right Eye',\n","    'H1CD0076': 'Disc. Abnormality-Left Eye',\n","    'H1CD0077': 'Lens Opacities-Right Eye',\n","    'H1CD0078': 'Lens Opacities-Left Eye',\n","    'H1CD0079': 'Ocular Fundi-Other-Right Eye',\n","    'H1CD0080': 'Ocular Fundi-Other -Left Eye',\n","    'H1CD0081': 'Neck-Venous Engorgement',\n","    'H1CD0082': 'Peripheral Arteries-All Normal',\n","    'H1CD0083': 'Periphearal Arteries-Right Side-Supe ...',\n","    'H1CD0084': 'Periphearal Arteries-Left Side-Brachial ',\n","    'H1CD0085': 'Peripheral Arteries-Right Side-Radial',\n","    'H1CD0086': 'Peripheral Arteries-Left Side Superf ...',\n","    'H1CD0087': 'Peripheral Arteries-Left Side  Brachial',\n","    'H1CD0088': 'Peripheral Arteries-Left Side-Radical',\n","    'H1CD0089': 'Quality of Aterial Pulsations-All Normal',\n","    'H1CD0090': 'Quality of Arterial Pulsations-Right 1...',\n","    'H1CD0091': 'Quality of Arterial Pulsations-Right 2...',\n","    'H1CD0092': 'Quality of Arterial Pulsations-Right 3...',\n","    'H1CD0093': 'Quality of Arterial Pulsations-Left  1...',\n","    'H1CD0094': 'Quality of Arterial Pulsations-Left  2...',\n","    'H1CD0095': 'Quality of Arterial Pulsations-Left  3...',\n","    'H1CD0096': 'Lower Extremities-Right',\n","    'H1CD0097': 'Lower Extremities-Left',\n","    'H1CD0098': 'Lower Extremities-Right-Varicosities',\n","    'H1CD0099': 'Lower Extremities-Right-Dependent Edema',\n","    'H1CD0100': 'Lower Extremities-Right Ulcers',\n","    'H1CD0101': 'Lower Extremities-Left-Varicosities',\n","    'H1CD0102': 'Lower Extremities-Left-Dependent Edema',\n","    'H1CD0103': 'Lower Extremities-Left-Ulcers',\n","    'H1CD0104': 'Thrills',\n","    'H1CD0105': 'Apical Impulse',\n","    'H1CD0106': 'Apical Impulse-Interspace (As Given)',\n","    'H1CD0107': 'Heart Sounds-Normal Chocked',\n","    'H1CD0108': 'Heart Sounds-A2',\n","    'H1CD0109': 'Heart Sounds-P2',\n","    'H1CD0110': 'Heart Sounds-M1',\n","    'H1CD0111': 'Heart Sounds-Third Heart Sound',\n","    'H1CD0112': 'Heart Sounds-Spilling of Second Soun ...',\n","    'H1CD0113': 'Heart Sounds-Other',\n","    'H1CD0114': 'Significant Murmurs-Type of Systolic ...',\n","    'H1CD0115': \"SIGNIFICANT MURMURS- PHYSICIAN'S IMP ...\",\n","    'H1CD0116': \"EXAMINING PHYSICIAN'S IMPRESSION-HYP ...\",\n","    'H1CD0117': \"EXAMINING PHYSICIAN'S IMPRESSION-PER ...\",\n","    'H1CD0118': \"EXAMINING PHYSICIAN'S IMPRESSION-ORG ...\",\n","    'H1CD0119': \"EXAMINING PHYSICIAN'S IMPRESSION-ANG ...\",\n","    'H1CD0120': \"EKG-SUMMARY OF 3 READINGS 1\",\n","    'H1CD0122': \"EKG-SUMMARY OF 3 READINGS 2\",\n","    'H1CD0123': \"EKG-SUMMARY OF 3 READINGS 3\",\n","    'H1CD0124': \"EKG-SUMMARY OF 3 READINGS 4\",\n","    'H1CD0125': \"EKG-SUMMARY OF 3 READINGS 5\",\n","    'H1CD0126': \"EKG-SUMMARY OF 3 READINGS 6\",\n","    'H1CD0127': \"EKG-SUMMARY OF 3 READINGS 7\",\n","    'H1CD0128': \"EKG-SUMMARY OF 3 READINGS 8\",\n","    'H1CD0129': \"EKG-SUMMARY OF 3 READINGS 9\",\n","    'H1CD0130': \"EKG-SUMMARY OF 3 READINGS 10\",\n","    'H1CD0131': \"EKG-SUMMARY OF 3 READINGS 11\",\n","    'H1CD0132': \"EKG-READING 1 1\",\n","    'H1CD0134': \"EKG-READING 1 2\",\n","    'H1CD0135': \"EKG-READING 1 3\",\n","    'H1CD0136': \"EKG-READING 1 4\",\n","    'H1CD0137': \"EKG-READING 1 5\",\n","    'H1CD0138': \"EKG-READING 1 6\",\n","    'H1CD0139': \"EKG-READING 1 7\",\n","    'H1CD0140': \"EKG-READING 1 8\",\n","    'H1CD0141': \"EKG-READING 1 9\",\n","    'H1CD0142': \"EKG-READING 1 10\",\n","    'H1CD0143': \"EKG-READING 1 11\",\n","    'H1CD0144': \"EKG-READING 2 1\",\n","    'H1CD0145': \"EKG-READING 2 2\",\n","    'H1CD0146': \"EKG-READING 2 3\",\n","    'H1CD0147': \"EKG-READING 2 4\",\n","    'H1CD0148': \"EKG-READING 2 5\",\n","    'H1CD0149': \"EKG-READING 2 6\",\n","    'H1CD0150': \"EKG-READING 2 7\",\n","    'H1CD0151': \"EKG-READING 2 8\",\n","    'H1CD0152': \"EKG-READING 2 9\",\n","    'H1CD0153': \"EKG-READING 2 10\",\n","    'H1CD0154': \"EKG-READING 2 11\",\n","    'H1CD0155': \"EKG-READING 2 12\",\n","    'H1CD0156': \"EKG-READING 3 1\",\n","    'H1CD0158': \"EKG-READING 3 2\",\n","    'H1CD0159': \"EKG-READING 3 3\",\n","    'H1CD0160': \"EKG-READING 3 4\",\n","    'H1CD0161': \"EKG-READING 3 5\",\n","    'H1CD0162': \"EKG-READING 3 6\",\n","    'H1CD0163': \"EKG-READING 3 7\",\n","    'H1CD0164': \"EKG-READING 3 8\",\n","    'H1CD0165': \"EKG-READING 3 9\",\n","    'H1CD0166': \"EKG-READING 3 10\",\n","    'H1CD0167': \"EKG-READING 3 11\",\n","    'H1CD0168': \"EKG-READING-AVERAGE RATE\",\n","    'H1CD0171': \"EKG READING-AVERAGE PR INTERVAL\",\n","    'H1CD0173': \"EKG READING-AVERAGE INTERVAL\",\n","    'H1CD0175': \"S-SAVE DEPTH IN V1 (SV1)\",\n","    'H1CD0178': \"R-SAVE DEPTH IN V5 (SVS)\",\n","    'H1CD0181': \"CHEST X-RAY-EXISTENCE OF LESION-READ 1...\",\n","    'H1CD0182': \"CHEST X-RAY-HEART ENLARGEMENT-READING 1\",\n","    'H1CD0183': \"CHEST X-RAY-OTHER CV ABNORMALITY-REA 1...\",\n","    'H1CD0185': \"CHEST X-RAY-EXISTENCE OF LESION-READ 2...\",\n","    'H1CD0186': \"CHEST X-RAY-HEART ENLARGEMENT-READING 2\",\n","    'H1CD0187': \"CHEST X-RAY-OTHER CV ABNORMALITY-REA 2...\",\n","    'H1CD0189': \"CHEST X-RAY-EXISTENCE OF LESION-READ 3...\",\n","    'H1CD0190': \"CHEST X-RAY-HEART ENLARGEMENT-READING 3\",\n","    'H1CD0191': \"CHEST X-RAY-OTHER CV ABNORMALITY-REA 3...\",\n","    'H1CD0193': \"CHEST X-RAY-FINAL EVALUATION OF HEAR ...\",\n","    'H1CD0194': \"SERUM CHOLESTEROL VALUES (MG%)\",\n","    'H1CD0196': \"STROKE DIAGNOSIS\",\n","    'H1CD0197': \"CARDIOVASCULAR CONDITION DIAGNOSIS\",\n","    'H1CD0198': \"HEART CONDITION DIAGNOSIS\",\n","    'H1CD0199': \"HIGH BLOOD PRESSURE DIAGNOSIS\",\n","    'H1CD0200': \"VARICOSE VEINS DIAGNOSIS\",\n","    'H1CD0201': \"CORONARY HEART DISEASE DIAGNOSIS 1\",\n","    'H1CD0202': \"RHEUMATIC HEART DISEASE DIAGNOSIS\",\n","    'H1CD0203': \"HYPERTENSIVE HEART DISEASE DIAGNOSIS\",\n","    'H1CD0204': \"HEART DISEASE SUMMARY (IN DESCENDING ...\",\n","    'H1CD0205': \"BLOOD PRESSURE SUMMARY\",\n","    'H1CD0206': \"HYPERTENSIVE HEART DISEASE SUMMARY\",\n","    'H1CD0207': \"RHEUMATIC HEART DISEASE SUMMARY\",\n","    'H1CD0208': \"CORONARY HEART DISEASE SUMMARY\",\n","    'H1CD0209': \"OTHER HEART DISEASE SUMMARY\",\n","    'H1CD0210': \"HEART DISEASE SUMMARY\",\n","    'H1CD0211': \"POSSIBLE HEART DISEASE\",\n","    'H1CD0212': \"HEART DISEASE DIAGNOSIS\",\n","    'H1CD0213': \"CORONARY HEART DISEASE DIAGNOSIS\",\n","    'H1CD0214': \"HYPERTENSIVE HEART DISEASE DIAGNOSIS 2\",\n","    'H1CD0215': \"HYPERTENSION DIAGNOSIS\",\n","    'H1CD0216': \"HYPERTENSION DIAGNOSIS BY PERSONAL P ...\",\n","    'H1CD0217': \"PERIPHERAL VASCULAR DISEASE  DIAGNOS ...\",\n","    'H1CD0218': \"CORONARY HEART DISEASE DIAGNOSIS BY  ...\",\n","    'H1CD0219': \"HYPERTENSIVE HEAR DISEASE DIAGNOSIS  ...\",\n","    'H1CD0220': \"RHEUMATIC HEAR DISEASE DIAGNOSIS BY  ...\",\n","    'H1CD0221': \"OTHER HEART DISEASE DIAGNOSIS BY PER ...\",\n","    'H1CD0222': \"UNUSED-ALL BLANKS\"\n","}\n","\n","# rename columns using the dictionary\n","df = df.rename(columns=column_mapping)\n","df = df.drop(['UNUSED-ALL BLANKS'], axis=1) # remove unused column\n","df = df.drop(['EKG-READING 2 1'], axis=1) # drops the extra column, seems to be a 'whether or not they got it' column\n","\n","numerical_columns = ['Time of Examination(01-21)','Average Systolic Blood Pressure','Average Diastolic Blood Pressure',\"EKG-READING-AVERAGE RATE\",\"EKG READING-AVERAGE PR INTERVAL\",\"EKG READING-AVERAGE INTERVAL\",\"S-SAVE DEPTH IN V1 (SV1)\",\"R-SAVE DEPTH IN V5 (SVS)\"]\n","\n","for column in numerical_columns:\n","    df[column] = df[column].astype(int)  # Convert to int\n","    df[column] = df[column].replace(999, pd.NA) # Replace 999s with NaN, more info in documentation of dataset\n","    df[column] = df[column].replace(99 , pd.NA) # were about 25 empty spots with &, replaced in excel with 999\n","\n","df[numerical_columns] = df[numerical_columns].fillna(df.median())  # Replace NaN with median of each column\n","#print(df[numerical_columns].median()) # print median of each column\n","\n","\n","ndf = df.copy() # create a copy of the dataframe to add names to, use df for ML\n","#df.to_csv('readdata.csv', index=False) # save to csv file"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def dictmerge(d1, d2): # dictionaries cannot be concat, so this is a workaround\n","    d3 = d1.copy()\n","    d3.update(d2)\n","    return d3"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# creates a dict for all categoricals, not strictly necessary but makes it easier to read later\n","# copilot rewrite: keeping comments, make codes a dict with integer keys \n","\n","# defines different meanings of catergoricals, originally done in lists but some are switched to dict as they were causing problems\n","codes3  = {1:'Yes, every few days, severe',2:'Yes, every few days, not severe',3:'Yes, less often, severe',\n","        4:'Yes, less often, not severe',5:'No or ?',7:'other'} # headaches, 7 instead of 9? not sure why\n","codes4  = {1:'Yes, every few days, severe',2:'Yes, every few days, not severe',3:'Yes, less often, severe',\n","        4:'Yes, less often, not severe',5:'No or ?',9:'other'} # nose bleeds, tinnitus, dizziness, fainting, sore throats etc\n","codes5  = codes4\n","codes6  = codes4 \n","codes7  = {1:'Yes',2:'No',3:'?',9:'Other'}\n","codes8  = {1: 'Yes, past 12 months, saw doctor', 2: 'Yes, past 12 months, no doctor or ?', \n","                  3: 'Yes, not in past 12 months, saw doctor', 4: 'Yes, not in past 12 months, no doctor or ?', \n","                  5: 'No or ?', 9:'other'} # stroke diagnosis\n","codes9  = {1: 'History and physical findings', 2: 'History only', 3: 'Negative'} # stroke review summary\n","codes10 = {1:'Yes',2:'No',3:'?',9:'Other'}\n","codes11 = codes10\n","codes12 = codes4\n","codes13 = codes4\n","codes14 = codes4\n","codes15 = codes4\n","codes16 = codes4\n","codes17 = codes4\n","codes18 = {1: 'Front', 2: 'Back', 3: 'Right Side', 4: 'Middle', 5: 'Left Side', 6: 'Somewhere Else', 7: 'More than one place', 9: 'Other', 0: 'No chest pain'} # chest pain location\n","codes19 = {1: 'Stays in one place', 2: 'Moves around', 3: '?', 9: 'Other', 0: 'No chest pain'}\n","codes20 = {1: 'Just a few minutes', 2: 'Five minutes to an hour', 3: 'More than an hour', 9: 'Other', 0: 'No chest pain'}\n","codes21 = {1: 'When exercising', 2: 'When quiet', 3: 'Makes no difference', 9: 'Other', 0: 'No chest pain'} # chest pain trigger\n","codes22 = {1: 'When upset', 2: 'Makes no difference', 9: 'Other', 0: 'No chest pain'} # chest pain trigger 2\n","codes23 = {1: 'Yes', 2: 'No', 3: '?', 9: 'Other', 0: 'No chest pain'}\n","codes24 = codes4\n","codes25 = codes18\n","codes26 = codes19\n","codes27 = codes20\n","codes28 = codes21\n","codes29 = codes22\n","codes30 = codes23\n","codes31 = codes4\n","codes32 = codes4\n","codes33 = {1: 'Yes, gone by morning', 2: 'Yes, not gone by morning', 3: 'Yes, ? gone by morning', 4: 'No', 5: '?', 9: 'Other'}\n","codes34 = codes4\n","codes35 = codes8 # varicose veins diagnosis\n","codes36 = {1: 'Yes, past 12 months, taking medication', 2: 'Yes, past 12 months, no medication or ?', 3: 'Yes, not in past 12 months, taking medication', 4: 'Yes, not in past 12 months, no medication or ?', 5: 'No or ?'} # Rheumatic fever diagnosis\n","codes37 = {1: 'Yes', 2: 'No', 3: 'Other'} # Chorea or St. Vitus Dance diagnosis\n","codes38 = {1: 'Yes, within past 12 months', 2: 'Yes, not within past 12 months', 3: 'Yes, ? within past 12 months', 4: 'No', 5: '?', 9: 'Other'} # Hardening of the arteries diagnosis\n","codes39 = {1: 'Yes, confirmed by doctor', 2: 'Yes, not confirmed by doctor', 3: 'No', 4: '?', 9: 'Other'} # High blood pressure - think you have it?\n","codes40 = {1: '1 year', 2: '1-5 years', 3: '>5 years', 9: 'other', 0: 'No HPB'}\n","codes41 = {1: 'Yes', 2: 'No', 3: '?', 9: 'Other', 0: 'No HPB'}\n","codes42 = {1: 'Yes, taking medication', 2: 'Not taking medication', 3: '?', 9: 'Other', 0: 'No HPB'}\n","codes43 = codes39 # Heart trouble - think you have it?\n","codes44 = codes40 # Heart trouble - how long ago diagnosed?\n","codes45 = {1: 'Yes', 2: 'No', 3: '?', 9: 'Other', 0: 'No heart trouble'}\n","codes46 = {1: 'Yes, taking medication', 2: 'Not taking medication', 3: '?', 9: 'Other', 0: 'No heart trouble'}\n","codes47 = 0 # Time of examination, do not turn to categorical\n","codes48 = 0 # avg systolic blood pressure, do not turn to categorical\n","codes49 = 0 # avg diastolic blood pressure, do not turn to categorical\n","codes50 = {1: '<90', 2: '90-100', 3: '100-109', 4: '110-119', 5: '120-129', 6: '130-139', 7: '140-149', 8: '150-159', 9: '160-169', 10: '170-179', 11: '180-189', 12: '190-199', 13: '200-209', 14: '210-219', 15: '220-229', 16: '230-239', 17: '240-249', 18: '250-259', 19: '>260'} # Avg systolic blood pressure coded\n","codes51 = {1: '<50', 2: '50-54', 3: '55-59', 4: '60-64', 5: '65-69', 6: '70-74', 7: '75-79', 8: '80-84', 9: '85-89', 10: '90-94', 11: '95-99', 12: '100-104', 13: '105-109', 14: '110-114', 15: '115-119', 16: '120-124', 17: '125-129', 18: '130-134', 19: '>135'} # Avg diastolic blood pressure coded\n","codes52 = {1: 'Before first BP', 2: 'Between 1st and 2nd BP', 3: 'Between 2nd and 3rd BP', 4: 'After all BP', 0: 'other'} # When venipuncture was done in relation to BP readings\n","codes53 = {1:'Normal',2:'Fundus not visualized',3:'Globe absent',4:'None of the above'} # Ocular fundi - right eye\n","codes54 = codes53 # Ocular fundi - left eye\n","codes55 = {1: 'Papilledepa', 2: 'Hemmorage and/or Exudates', 3: 'Other positives', 4: 'No abnormality', 5: 'None/Unknown'}\n","codes56 = codes55 # Ocular fundi condition - left eye\n","codes57 = {1:'Present',2:'Not present'} # Venous engorgement - right eye\n","codes58 = codes57 # Venous engorgement - left eye\n","codes59 = codes57 # Disc abnormality - right eye\n","codes60 = codes57 # Disc abnormality - left eye\n","codes61 = codes57 # Lens opacities - right eye\n","codes62 = codes57 # Lens opacities - left eye\n","codes63 = codes57 # Ocular fundi - other - right eye\n","codes64 = codes57 # Ocular fundi - other - left eye\n","codes65 = {1:'Yes',2:'No',9:'other',0:'no entry'} # Neck - venous engorgement\n","codes66 = {1:'Yes',2:'No',9:'other',0:'no entry'}  # Peripheral arteries - all normal\n","codes67 = {1: 'Normal', 2: 'Scleratic only', 3: 'Tortucus only', 4: 'Both', 5: 'Not done', 9: 'other', 0: 'All normal'} # Peripheral arteries - right side - superficial temporal\n","codes68 = codes67 # Peripheral arteries - right brachial\n","codes69 = codes67 # Peripheral arteries - right radial\n","codes70 = codes67 # Peripheral arteries - left superficial temporal\n","codes71 = codes67 # Peripheral arteries - left brachial\n","codes72 = codes67 # Peripheral arteries - left radial\n","codes73 = {1:'Yes',2:'No'} # Quality of arterial pulsations - all normal\n","codes74 = {0: 'All Normal', 1: 'Norral(?)', 2: 'Bounding', 3: 'Diminished', 4: 'Not palpable', 5: 'Not done', 9: 'other'}\n","codes75 = codes74 # Peripheral arteries - right side - dorsalis pedis\n","codes76 = codes74 # Peripheral arteries - right side - posterior tibial\n","codes77 = codes74 # Peripheral arteries - left side - radial\n","codes78 = codes74 # Peripheral arteries - left side - dorsalis pedis\n","codes79 = codes74 # Peripheral arteries - left side - posterior tibial\n","codes80 = {1:'Normal',2:'Not done',3:'Neither'} # Lower Extremities - right\n","codes81 = codes80 # Lower Extremities - left\n","codes82 = {1:'Checked',2:'Not checked',3:'All normal'} # Lower Extremities - right - varicosities\n","codes83 = codes82 # Lower Extremities - right - dependent edema\n","codes84 = codes82 # Lower Extremities - right - ulcers\n","codes85 = codes82 # Lower Extremities - left - varicosities\n","codes86 = codes82 # Lower Extremities - left - dependent edema\n","codes87 = codes82 # Lower Extremities - left - ulcers\n","codes88 = {1: 'Aortic Systolic', 2: 'Apical Systolic', 3: 'Apical Diastolic', 4: 'Pulmonic Systolic', 5: 'other', 6: 'None', 0: 'No entry'} # Heart thrills\n","codes89 = {1: 'Not felt', 2: 'MCL at or inside', 3: 'MCL outside', 0: 'No entry'} # Apical impulse\n","codes90 = {1: '3', 2: '4', 3: '5', 4: '6', 5: '7', 0: 'No entry'} # Apical impulse - interspace\n","codes91 = {1:'Normal',2:'No, no other entries checked',3:'No, other entries checked'} # Heart sounds - normal checked (like the box)\n","codes92 = {1:'Accentuated',2:'Diminished',3:'No entry',0:'All normal'} # Heart sounds - A2\n","codes93 = codes92 # Heart sounds - P2\n","codes94 = codes92 # Heart sounds - M1\n","codes95 = {1:'Present',2:'Absent',0:'All normal'} # Heart sounds - third heart sound\n","codes96 = codes95 # Heart sounds - spilling of second sound abnormal\n","codes97 = codes95 # Heart sounds - other (like other things present)\n","codes98 = {1: 'Grade 3', 2: 'Grade 4', 3: 'Murmur with thrill at base', 4: 'Others', 0: 'None'}  # Significant murmurs - type of systolic murmur\n","codes99 = {1: 'Pheumatic', 2: 'Congenital', 3: 'Aortic Stenosis', 4: 'Other', 5: 'No HD', 0: 'If codes 4,0 in item 98'}\n","codes100 = {1:'Positive',2:'Negative',3:'Suspect',0:'No entry'} # Examining physician's impression - hypertenion\n","codes101 = codes100 # Examining physician's impression - peripheral arteriosclerosis\n","codes102 = codes100 # Examining physician's impression - organic heart disease\n","codes103 = codes100 # Examining physician's impression - angina pectoris\n","codes104 = {\n","    1: 'Normal EKG',\n","    2: 'Unsatisfactory EKG',\n","    3: 'No myocardial infarction',\n","    4: 'Anterior myocardial infarction',\n","    5: 'Posterior myocardial infarction',\n","    6: 'Anterior myocardial infarction outside criteria',\n","    7: 'Posterior myocardial infarction outside criteria',\n","    8: 'Ant and Post Myocardial Infarction',\n","    10: 'other'\n","} # EKG - summary of 3 readings\n","\n","# for below bits\n","code105 = {1:'No axis deviation',2:'Left axis deviation',3:'Right axis deviation',4:'Left axis outside criteria',5:'Right axis outside criteria'} \n","code106 = {1:'No ventricular hypertrophy',2:'Left ventricular hypertrophy',3:'Right ventricular hypertrophy',4:'Left VH outside criteria',\n","        5:'Right \" \" outside criteria',6:'Both L/R VH'}\n","code107 = {\n","    1: 'No abnormalities on ST segment or junction',\n","    2: 'Subendocardial Ischemia',\n","    3: 'Subendocardial Ischemia/digitalis',\n","    4: 'Current of injury',\n","    5: 'Subendocardial ischemia outside criteria',\n","    6: 'Subendocardial Ischemia/digitalis outside criteria',\n","    7: 'Current of injury outside criteria',\n","    8: 'other'\n","}\n","code108 = {\n","    1: 'No abnormal T-wave',\n","    2: 'Non-specific T-Wave',\n","    3: 'Left ventricular ischemia',\n","    4: 'Non-specific T-wave, outside criteria',\n","    5: 'Left vent. Ischemia outside criteria'\n","}\n","code109 = {\n","    1: 'No AV conduction abnormalities',\n","    2: 'Complete AV block',\n","    3: 'Partial AV block',\n","    4: 'First degree AV block',\n","    5: 'WPW',\n","    6: 'WPW outside criteria',\n","    7: 'other'\n","}\n","code110 = {\n","    1: 'No ventricular conduction abnormalities',\n","    2: 'Left bundle branch block',\n","    3: 'Right bundle branch block',\n","    4: 'Increased right bundle branch block',\n","    5: 'I-V block',\n","    6: 'Left bundle block and I-V block',\n","    7: 'Incr. right bundle branch block outside criteria',\n","    8: 'other'\n","}\n","code111 = {\n","    1: 'No arrhythmias',\n","    2: 'Ventricular tachycardia',\n","    3: 'Auricular fibrillation',\n","    4: 'Aur. Hod(?). or supra-vent tachycardia',\n","    5: 'Abnormal ventricular rhythm',\n","    6: 'Abnormal nodal rhythm',\n","    7: 'other',\n","}\n","code112 = {\n","    1: 'Neither low QRS or high T-wave',\n","    2: 'Low QRS',\n","    3: 'High T-wave',\n","    4: 'Both'\n","}  # EKG - summary of 3 readings \n","code113 = {\n","    1: 'No premature contractions',\n","    2: 'Rare atrial premature contractions',\n","    3: 'Rare ventricular premature contractions',\n","    4: 'Rare nodal premature contractions',\n","    5: 'Frequent atrial premature contractions',\n","    6: 'Frequent ventricular premature contractions',\n","    7: 'Frequent nodal premature contractions',\n","    8: 'other'\n","}\n","code114 = {1:'No misc findings',2:'Misc findings'} # EKG - summary of 3 readings # EKG - reading 1\n","code115 = dictmerge(codes104, {11:'Not examined by this reader'}) # EKG - summary of 3 readings\n","code116 = dictmerge(code105, {0:'Codes 1,2,10,or 11 in item 115'}) # EKG - summary of 3 readings\n","# continue\n","\n","matching104 = {0:'Codes 1,2,or 10 in item 104'}\n","matching115 = {0:'Codes 1,2,10,or 11 in item 115'}\n","matching126 = {0:'Codes 1,2,10,or 11 in item 126'}\n","matching137 = {0:'Codes 1,2,10,or 11 in item 137'}\n","codes105 = dictmerge(matching104, code105) # EKG - summary of 3 readings\n","codes106 = dictmerge(matching104, code106) # EKG - summary of 3 readings\n","codes107 = dictmerge(matching104, code107) # EKG - summary of 3 readings\n","codes108 = dictmerge(matching104, code108) # EKG - summary of 3 readings\n","codes109 = dictmerge(matching104, code109) # EKG - summary of 3 readings\n","codes110 = dictmerge(matching104, code110) # EKG - summary of 3 readings\n","codes111 = dictmerge(matching104, code111) # EKG - summary of 3 readings\n","codes112 = dictmerge(matching104, code112) # EKG - summary of 3 readings\n","codes113 = dictmerge(matching104, code113) # EKG - summary of 3 readings\n","codes114 = dictmerge(matching104, code114) # EKG - summary of 3 readings\n","codes115 = code115 # EKG - reading 1\n","codes116 = dictmerge(matching115, code105) # EKG - reading 1\n","codes117 = dictmerge(matching115, code106) # EKG - reading 1\n","codes118 = dictmerge(matching115, code107) # EKG - reading 1\n","codes119 = dictmerge(matching115, code108) # EKG - reading 1\n","codes120 = dictmerge(matching115, code109) # EKG - reading 1\n","codes121 = dictmerge(matching115, code110) # EKG - reading 1\n","codes122 = dictmerge(matching115, code111) # EKG - reading 1\n","codes123 = dictmerge(matching115, code112) # EKG - reading 1\n","codes124 = dictmerge(matching115, code113) # EKG - reading 1\n","codes125 = dictmerge(matching115, code114) # EKG - reading 1\n","codes126 = dictmerge(codes104, {11:'Not examined by this reader'}) # EKG - reading 2\n","codes127 = dictmerge(matching126, codes105) # EKG - reading 2\n","codes128 = dictmerge(matching126, codes106) # EKG - reading 2\n","codes129 = dictmerge(matching126, codes107) # EKG - reading 2\n","codes130 = dictmerge(matching126, codes108) # EKG - reading 2\n","codes131 = dictmerge(matching126, codes109) # EKG - reading 2\n","codes132 = dictmerge(matching126, codes110) # EKG - reading 2\n","codes133 = dictmerge(matching126, codes111) # EKG - reading 2\n","codes134 = dictmerge(matching126, codes112) # EKG - reading 2\n","codes135 = dictmerge(matching126, codes113) # EKG - reading 2\n","codes136 = dictmerge(matching126, codes114) # EKG - reading 2\n","codes137 = dictmerge(codes104, {11:'Not examined by this reader'}) # EKG - reading 3\n","codes138 = dictmerge(matching137, codes105) # EKG - reading 3\n","codes139 = dictmerge(matching137, codes106) # EKG - reading 3\n","codes140 = dictmerge(matching137, codes107) # EKG - reading 3\n","codes141 = dictmerge(matching137, codes108) # EKG - reading 3\n","codes142 = dictmerge(matching137, codes109) # EKG - reading 3\n","codes143 = dictmerge(matching137, codes110) # EKG - reading 3\n","codes144 = dictmerge(matching137, codes111) # EKG - reading 3\n","codes145 = dictmerge(matching137, codes112) # EKG - reading 3\n","codes146 = dictmerge(matching137, codes113) # EKG - reading 3\n","codes147 = dictmerge(matching137, codes114) # EKG - reading 3\n","codes148 = 0 # avg rate, do not turn to categorical\n","codes149 = 0 # avg PR interval, do not turn to categorical\n","codes150 = 0 # avg QRS interval, do not turn to categorical\n","codes151 = 0 # S wave depth in V1, do not turn to categorical\n","codes152 = 0 # R wave depth in V5, do not turn to categorical\n","codes153 = {1:'No lesion',2:'Indefinite',3:'Definite',0:'Unsatisfactory or missing X-Ray'} # Chest X-Ray - existence of lesion\n","codes154 = {\n","    1: 'None',\n","    2: 'Definite enlargement',\n","    3: 'Borderline enlargement',\n","    4: 'Definite LV',\n","    5: 'Borderline LV',\n","    6: 'Definite other chamber',\n","    7: 'Borderline other chamber',\n","    8: 'Definite combination of others',\n","    9: 'Borderline combination of others',\n","    0: 'Unsatisfactory or missing XRay'\n","}\n","codes155 = {\n","    1: 'None',\n","    2: 'Definite Calcification(aorta)',\n","    3: 'Borderline Calcification(aorta)',\n","    4: 'Definite shape of aorta',\n","    5: 'Borderline shape of aorta',\n","    6: 'Definite calcification and shape',\n","    7: 'Borderline calcification and shape',\n","    8: 'Definite inc. pulmonary vascularity',\n","    9: 'Borderline inc. pulmonary vascularity',\n","    10: 'Definite IPV and calcification or shape of aorta',\n","    11: 'Borderline IPV and calcification or shape of aorta',\n","    0: 'Unsatisfactory or missing XRay'\n","}\n","codes156 = codes153 # Chest X-Ray - existence of lesion reading 2\n","codes157 = codes154 # Chest X-Ray - existence of heart enlargement reading 2\n","codes158 = codes155 # Chest X-Ray - other cardiovascular abnormality reading 2\n","codes159 = codes153 # Chest X-Ray - existence of lesion reading 3\n","codes160 = codes154 # Chest X-Ray - existence of heart enlargement reading 3\n","codes161 = codes155 # Chest X-Ray - other cardiovascular abnormality reading 3\n","codes162 = {0:'Negative',1:'Definite 1',2:'Definite 2',3:'Definite 3',4:'Definite 4',5:'Definite 5',6:'Borderline 6',7:'Borderline 7',8:'Unsatisfactory or missing XRay'} # Chest X-Ray - existence of heart enlargement\n","codes163 = {\n","    1: '<80',\n","    2: '80-99',\n","    3: '100-119',\n","    4: '120-139',\n","    5: '140-159',\n","    6: '160-179',\n","    7: '180-199',\n","    8: '200-219',\n","    9: '220-239',\n","    10: '240-259',\n","    11: '260-279',\n","    12: '280-299',\n","    13: '300-319',\n","    14: '320-339',\n","    15: '340-359',\n","    16: '360-379',\n","    17: '380-399',\n","    18: '400-419',\n","    19: '420-439',\n","    20: '440-459',\n","    21: '460-479',\n","    22: '480-499',\n","    23: '500-519',\n","    24: '>520',\n","    99: 'unknown'\n","}\n","codes164 = codes73 # stroke diagnosis\n","codes165 = codes73 # cardiovascular condition diagnosis\n","codes166 = codes73 # heart condition diagnosis\n","codes167 = codes73 # high blood pressure diagnosis\n","codes168 = codes73 # varicose veins diagnosis\n","codes169 = codes73 # coronary heart disease diagnosis\n","codes170 = codes73 # rheumatic heart disease diagnosis\n","codes171 = codes73 # hypertensive heart disease diagnosis\n","codes172 = {1:'Definite - Aortic stenosis',2:'Definite - other',3:'Suspect - Definite enlargement on chest X-Ray',\n","            4:'Av block, right bundle block, right or left or both hypertrophy',5:'No HD'} # heart disease summary\n","codes173 = {\n","    1: 'Definite hypertension',\n","    2: 'Borderline hypertension',\n","    3: 'No hypertension',\n","    4: 'No entry'\n","}\n","codes174 = {\n","    1: 'Definite hypertension, LVH, and 35+',\n","    2: 'Definite hypertention, enlargement in XRay',\n","    3: 'Suspect hypertention, borderline HT and enlargement in XRay',\n","    4: 'Suspect hypertention, borderline HT and LVH and 35+',\n","    5: 'Borderline HT and definite enlargement in X-Ray',\n","    6: 'No HD',\n","    7: 'No hypertensive HD',\n","    8: 'Definite, under tratment for HT & LVH and 35+',\n","    9: 'Definite, under treatment for HT and enlargement in XRay'\n","}\n","codes175 = {\n","    1: 'Diastolic murmur',\n","    2: 'Grade 4 systolic murmur and Rheumatic HD',\n","    3: 'Grade3 systolic murmur and Rheumatic HD with hist of RHD or chorea',\n","}\n","codes176 = {\n","    1: 'Definite - Myocardial infarction',\n","    2: 'Definite - angina pectoris',\n","    3: 'Definite - L vent. ischemia',\n","    4: 'Suspect - Angina pectoris',\n","    5: 'No Coronary HD',\n","}\n","codes177 = {\n","    1: 'Congenital heart disease',\n","    2: 'Syphilitic heart disease',\n","    3: 'Traumatic heart disease',\n","    4: 'No heart disease',\n","}\n","codes178 = {\n","    1: 'Definite hypertensive, rheumatic, coronary, or other HD',\n","    2: 'Suspect HT coronary or other HD',\n","    3: 'No HD',\n","}\n","codes179 = {\n","    1: 'EKG abnormalities',\n","    2: 'Left axis deviation and corr. history',\n","    3: '1st degree AV block and corr. history',\n","    4: 'Venous engor., thrils, sig. sys. murmurs, splitting of second sound, other heart sound',\n","    5: 'No HD diagnosis/Code 0 in items 174,175,176,or 177',\n","} # Rhermatic heart disease summary\n","codes180 = {1:'Exam +, Physician +',2:'Exam +, Physician -',3:'Exam -, Physician +',4:'Exam -, Physician -',5:'Exam +, no Phys',6:'Exam -, no Phys',0:'other'} # heart disease diagnosis\n","codes181 = codes180 # coronary heart disease diagnosis\n","codes182 = codes180 # hypertensive heart disease diagnosis\n","codes183 = codes180 # hypertension diagnosis\n","codes184 = {1:'Positive',2:'Negative',3:'?',4:'N, A, Unknown, DK',0:'No physician exam'} # hypertension diagnosis by personal physician\n","codes185 = codes184 # peripheral vascular disease diagnosis\n","codes186 = codes184 # coronary heart disease diagnosis by personal physician\n","codes187 = codes184 # hypertensive heart disease diagnosis by personal physician\n","codes188 = codes184 # rheumatic heart disease diagnosis by personal physician\n","codes189 = codes184 # other heart disease diagnosis by personal physician\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Elapsed time: 2.978886842727661 seconds\n"]}],"source":["# names categoricals\n","for column in ndf.columns[2:]:\n","    code = eval('codes'+str(ndf.columns.get_loc(column)+1))\n","    #print('codes'+str(ndf.columns.get_loc(column)+1), column)\n","    #print(sorted(df[column].unique()))\n","    #if column == 'EKG-READING 2 1': # skips this one because it's messed up, drop later? - dropped now earlier in code, now data lines up\n","        #continue\n","    if code == 0: # skips all codes that = 0\n","        continue\n","    # elif len(code) != len(df[column].unique()): # checks that we have the same number of codes as unique values\n","    #    raise ValueError('Error in column: '+str(column)+'. Length mismatch in '+str('codes'+str(df.columns.get_loc(column)+1))\n","    #                    +'. There are '+str(len(code))+' codes and '+str(len(df[column].unique()))+' values. '\n","    #                    + '\\nUnique values: '+str(sorted(df[column].unique()))+'.'\n","    #                    + '\\nUnique codes: '+str(code)+'.')\n","        # tells you if not and what to fix, deprecated as I changed the lists to dicts\n","    # turns to categorical if all others pass\n","    # print(df[column].unique())\n","    try:\n","        ndf[column] = pd.Categorical(ndf[column], categories=eval(str(sorted(ndf[column].unique()))), ordered=True)\n","        ndf[column] = ndf[column].cat.rename_categories(code)\n","    except NameError:\n","        print('Error in column '+column+':'+str(NameError))\n","    #print(df[column].unique())\n","    #print(df[column].value_counts())\n","    #print('\\n')\n","with open(r'readabledata.csv', 'w') as f:\n","    f.write(ndf.to_csv(index=False))\n","    \n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","print(f\"Elapsed time: {elapsed_time} seconds\")\n","\n","# item 126 might be a 'whether or not it was done', because it matches the 'not examined by this reader' number in \n","# the 1 category."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(6672,)\n","(6672, 188)\n","The training set has 5337 samples and the test set has 1335 samples.\n","Elapsed time: 14.540719509124756 seconds\n"]}],"source":["# import libraries from NN_MOF.py\n","import numpy as np\n","import random as rn\n","#import tensorflow as tf\n","\n","from sklearn.preprocessing import StandardScaler\n","\n","from sklearn.model_selection import train_test_split\n","\n","from imblearn.over_sampling import SMOTE\n","from sklearn.neural_network import MLPClassifier\n","\n","from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_score\n","\n","import matplotlib.pyplot as plt\n","\n","from sklearn.metrics import roc_curve, classification_report, confusion_matrix, auc, roc_auc_score\n","from sklearn import metrics\n","\n","seed = 5780\n","np.random.seed(seed)\n","rn.seed(seed)\n","\n","column_to_predict = 'HEART CONDITION DIAGNOSIS'\n","\n","# split into X and y\n","Y_data = df[column_to_predict]\n","X_data = df.drop([column_to_predict], axis=1)\n","Y_data = Y_data.astype('float32')\n","X_data = X_data.astype('float32')\n","\n","\n","print(Y_data.shape)\n","print(X_data.shape)\n","\n","#test and train sets, 80/20 split\n","X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, test_size=0.2, random_state=seed)\n","\n","print(f'The training set has {len(X_train)} samples and the test set has {len(X_test)} samples.')\n","\n","# standardize continuous variables here?\n","\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","print(f\"Elapsed time: {elapsed_time} seconds\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Before OverSampling, counts of label '1 (Yes)': 224\n","Before OverSampling, counts of label '0 (No)': 0 \n","\n","After OverSampling, counts of label '1': 5113\n","After OverSampling, counts of label '0': 0\n","Before OverSampling, the shape of train_X: (5337, 188)\n","Before OverSampling, the shape of train_y: (5337,) \n","\n","After OverSampling, the shape of train_X: (10226, 188)\n","After OverSampling, the shape of train_y: (10226,) \n","\n","Fitting 10 folds for each of 864 candidates, totalling 8640 fits\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n","c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n","c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n","c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n","c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n","c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n","c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n","c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n","c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n","c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n","c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n","c:\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n","  warnings.warn(\"Training interrupted by user.\")\n"]}],"source":["\n","nn = MLPClassifier(max_iter=100)\n","\n","# Hyperparameter search for MLP.\n","nn_param_search = {\n","    'hidden_layer_sizes': [(10,30,10),(20,), (30,), (40,), (50,), (60,), (70,), (80,)],\n","    'activation': ['tanh', 'relu', 'logistic', 'identity'],\n","    'solver': ['sgd', 'adam', 'lbfgs'],\n","    'alpha': [0.0001, 0.05, 0.1], # L2 reguarization\n","    'learning_rate': ['constant','adaptive','invscaling'],\n","    }\n","\n","# change verbose to any interger if you want to print out the message\n","nn_grid_search=GridSearchCV(nn, nn_param_search, n_jobs=1, cv=10, verbose=2) \n","\n","# n_jobs = -1 uses all processors, or you can specify the number of processors to use\n","\n","# SMOTE in training data\n","sm = SMOTE(sampling_strategy=1.0,random_state=seed)\n","x_train_s, y_train_s = sm.fit_resample(X_train, y_train)\n","\n","# Print before and after SMOTE\n","print(\"Before OverSampling, counts of label 'Hypertensive, rheumatic, coronary, or other heart disease': {}\".format(sum(y_train == 1)))\n","print(\"Before OverSampling, counts of label 'Suspect hypertensive, coronary, or other HD': {} \\n\".format(sum(y_train == 2)))\n","print(\"Before OverSampling, counts of label 'No HD': {} \\n\".format(sum(y_train == 3)))\n","    \n","print(\"After OverSampling, counts of label 'Hypertensive, rheumatic, coronary, or other heart disease': {}\".format(sum(y_train_s == 1)))\n","print(\"After OverSampling, counts of label 'Suspect hypertensive, coronary, or other HD': {}\".format(sum(y_train_s == 2)))\n","print(\"After OverSampling, counts of label 'No HD': {} \\n\".format(sum(y_train_s == 3))) \n","\n","print('Before OverSampling, the shape of train_X: {}'.format(X_train.shape))\n","print('Before OverSampling, the shape of train_y: {} \\n'.format(y_train.shape))\n","\n","print('After OverSampling, the shape of train_X: {}'.format(x_train_s.shape))\n","print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_s.shape))\n","\n","# Fit the model using training data after SMOTE\n","print('Grid Searching...')\n","nn_grid_search.fit(x_train_s, y_train_s)\n","print('Still Searching...')\n","nn_best_grid = nn_grid_search.best_estimator_\n","nn_best_params = nn_grid_search.best_params_\n","print('Grid Search Finished.')\n","nn_best_grid.save('nn_best_grid.h5')\n","\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","print(f\"Elapsed time: {elapsed_time} seconds\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["yscore_raw = nn_grid_search.predict_proba(X_test)\n","yscore = [s[1] for s in yscore_raw]\n","fpr, tpr, thresh = roc_curve(y_test, yscore)\n","\n","# Calculate the area under the ROC curve (AUC)\n","roc_auc = auc(fpr, tpr)\n","\n","# Plot the ROC curve\n","plt.figure(figsize=(8, 6))\n","plt.plot(fpr, tpr, label=f'Neural Network (AUC = {roc_auc:.2f})')\n","plt.plot([0, 1], [0, 1], 'k--')  # Plot the random guess line\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic (ROC) Curve')\n","plt.legend(loc=\"lower right\")\n","plt.savefig('Neural Network_MOF.png')\n","\n","# print the best hyperparameters\n","print(f'The best parameters are {nn_best_params}')\n","\n","y_hat = nn_best_grid.predict(X_test)\n","\n","# Calculate the confusion matrix\n","cm = confusion_matrix(y_test, y_hat)\n","\n","# Extract the true positives, true negatives, false positives, and false negatives\n","tn, fp, fn, tp = cm.ravel()\n","\n","# Calculate sensitivity (true positive rate)\n","sensitivity = tp / (tp + fn)\n","\n","# Calculate specificity (true negative rate)\n","specificity = tn / (tn + fp)\n","\n","# Calculate accuracy\n","accuracy = (tp + tn) / (tp + tn + fp + fn)\n","\n","# Print the sensitivity, specificity, and accuracy\n","print(f\"Sensitivity: {sensitivity:.2f}\") # \n","print(f\"Specificity: {specificity:.2f}\") # \n","print(f\"Accuracy: {accuracy:.2f}\") # \n","\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","print(f\"Elapsed time: {elapsed_time} seconds\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Neural Network\n","neuralnetwork = MLPClassifier(random_state=seed, max_iter=1000)\n","\n","nn_param_search = {\n","    'hidden_layer_sizes': [(10,30,10),(20,), (30,), (40,), (50,), (60,), (70,), (80,)],\n","    'activation': ['tanh', 'relu', 'logistic', 'identity'],\n","    'solver': ['sgd', 'adam', 'lbfgs'],\n","    'alpha': [0.0001, 0.05, 0.1], # L2 reguarization\n","    'learning_rate': ['constant','adaptive','invscaling'],\n","    }\n","\n","grid_search=GridSearchCV(neuralnetwork, nn_param_search, n_jobs=-1, cv=10, verbose=False) \n","\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","print(f\"Elapsed time: {elapsed_time} seconds\")"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1701570545.8317673\n"]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"nbformat":4,"nbformat_minor":2}
